{
  "hash": "f8e1e98cfe69eac22f413c3b28554433",
  "result": {
    "markdown": "---\ntitle: \"Regression\"\ndescription: |\n  Regression analysis is a statistical method that describes the relationship between a given predictor and outcome. In the case of multivariable regression, it describes that relationship after \"controlling\" for other variables. At the end of the session, you should be able to interpret the results from simple or multivariable regression (linear or logistic) analyses.\ndate: 07-16-2020\nauthor:\n  - name: SGH PT Dept\nbibliography: 'bibliography.bib'\ncategories:\n - regression\n - odds ratio\n - beta\nimage: 'images/ancova.png'\neditor: \n  markdown: \n    wrap: 120\n---\n\n\n\n\n## Definitions\n\n**linear regression model**: This is also called ordinary least squares (OLS) and refers to regression for a continuous\ndependent variable, and usually to the case where the residuals are assumed to be Gaussian.\n\n**multivariable model**: A model relating multiple predictor variables (risk factors, treatments, etc.) to a single\nresponse or dependent variable.\n\n**multivariate model**: A model that **simultaneously** predicts more than one dependent variable, e.g. a model to\npredict systolic and diastolic blood pressure or a model to predict systolic blood pressure 5 min. and 60 min. after\ndrug administration.\n\n<aside>Taken from Frank Harrell's [glossary](http://hbiostat.org/doc/glossary.pdf)</aside>\n\n## Regression Model\n\nThe choice of regression model depends on the nature of the outcome or dependent variable.\n\n-   Continuous outcome: linear regression model\n\n-   Binary outcome: binary logistic regression model\n\n-   Ordinal outcome: ordinal regression model\n\n-   Time to event outcome: Cox proportional hazards model[^1]\n\n[^1]: Not covered in this workshop\n\nRegression models are used for\n\n-   hypothesis testing\n\n-   estimation (with confounder adjustment to get adjusted estimates of effects)\n\n-   prediction\n\n### Notation\n\n$E(y|x) = \\alpha + \\beta x$ (simple regression)\n\n$E(y|x) = \\alpha + \\beta_1x_1 + \\beta_2x_2$ (multiple regression)\n\n$y$: Response, dependent, or outcome variable\n\n$x$: Predictor or independent variable\n\n$E(y|x)$ : Expected value of $y$ conditioned on the value of $x$\n\n$\\beta$: Slope of $y$ on $x$ ($\\frac{\\Delta{y}}{\\Delta{x}}$)\n\n<!-- $e$: random error = residual = variation in $y$ when holding $x$ constant (e.g., variation in gait speed [$y$] between patients of the same age [$x$]) -->\n\n<aside>June says I have maxed out my quota for equations and symbols</aside>\n\nWatch this video which explains the notation of simple linear regression\n\n<aside>Use this [link](https://www.youtube.com/watch?v=KsVBBJRb9TE&list=PLvxOuBpazmsND0vmkP1ECjTloiVz-pXla) if the video\ndoesn't play</aside>\n\n<iframe width=\"560\" height=\"350\" src=\"https://www.youtube.com/embed/KsVBBJRb9TE\" frameborder=\"0\" allowfullscreen>\n\n</iframe>\n\n### Interpretating $x$\n\n-   Generally speaking, $x$ is the predictor or independent variable - a variable that is associated with the outcome\n    ($y$)\n\n-   Depending on the study design, $x$ can also represent the explanatory variable, risk factor, confounder, covariate,\n    or covariable. With multiple $x$'s, they can be (i) multiple risk factors, (ii) treatment variable plus baseline\n    outcomes plus patient descriptors (age, gender), or (iii) different levels of a nominal (categorical) variable\n    (e.g., race)\n\n### Interpretating $\\beta$\n\n-   slope\n\n-   regression coefficient\n\n-   effect size of a given predictor\n\n-   effect of increasing $x$ by one unit on the change in the mean of $y$, holding all other $x$'s constant\n\n-   with multiple $x$'s, the $\\beta$s represent the *partial* effects of $x$s\n\n$E(y|x) = \\alpha + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3$\n\n<aside>Note: By holding all other $x$'s (confounders) constant, we are minimizing the risk of confounding!</aside>\n\n$\\beta_1$ interpretation: Holding $x_2$ and $x_3$ constant, how much does mean $y$ change when $x_1$ changes by 1 unit?\n\n$\\beta_2$ interpretation: Holding $x_1$ and $x_3$ constant, how much does mean $y$ change when $x_2$ changes by 1 unit?\n\n$\\beta_3$ interpretation: Holding $x_1$ and $x_2$ constant, how much does mean $y$ change when $x_3$ changes by 1 unit?\n\nIQR-$\\beta$ = change in the mean of $y$ per one *IQR* increase in $x$\n\nWatch this video which explains the interpretation of $\\beta$ from a simple linear regression model. Please note that\n$\\beta_0$ in the video refers to $\\alpha$ using our notation.\n\n<aside>Use this [link](https://www.youtube.com/watch?v=I8Dr1OGUdZQ&list=PLvxOuBpazmsND0vmkP1ECjTloiVz-pXla&index=3) if\nthe video doesn't play</aside>\n\n<iframe width=\"560\" height=\"350\" src=\"https://www.youtube.com/embed/I8Dr1OGUdZQ\" frameborder=\"0\" allowfullscreen>\n\n</iframe>\n\nWatch this [video](https://www.youtube.com/watch?v=dQNpSa-bq4M) which explains the interpretation of $\\beta$ from a\nmultivariable linear regression model. Please note that $\\beta_0$ in the video refers to $\\alpha$ using our notation. To\njump straight into $\\beta$ interpretation, start from minute $14$ of the video.\n\nLogistic regression = exponentiating $\\beta$ = [Odds Ratio](#lrm)\n\nCox proportional hazards regression = exponentiating $\\beta$ = Hazard Ratio (HR)\n\n## Linear Regression\n\n### Example 1\n\nExpected post-Rx disability = $24$ + $0.71\\times$baseline score + $12.7\\times$acupuncture\n\n-   Interpretation: **12.7** represents the difference between groups on mean change scores. Outcome score improved by\n    an estimated **12.7** points more on average in the acupuncture group than in the placebo group.\n\n<aside>In a research paper, the authors will report the $\\beta$ for the `acupuncture` (*12.7*) term</aside>\n\n-   An analysis of covariance (ANCOVA) adjusts each patient's follow up score for his or her baseline\n    score[@vickers2001ancova]\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The estimated difference between the groups from analysis of covariance is the vertical distance between the red and dotted lines. This figure is taken from Vickers et al [@vickers2001ancova]](images/ancova.png){fig-align='center' width=100%}\n:::\n:::\n\n\n<aside>Disclaimer: Don't freak out if you don't understand this. As mentioned, the most important result is *12.7* - the\n$\\beta$ for acupuncture</aside>\n\n-   **ANCOVA**: Analysis of covariance is *just* multiple regression (i.e., a linear model ) where one variable is of\n    major interest and is categorical (e.g., treatment group). In classic ANCOVA there is a treatment variable and a\n    continuous covariate used to reduce unexplained variation in the dependent variable, thereby increasing power.\n\n<aside>Taken from Frank Harrell's [glossary](http://hbiostat.org/doc/glossary.pdf)</aside>\n\n### Example 2\n\nTable 2, taken from our JoSPT paper[@pua2017associations], describes how to interpret the output from a linear and\n(ordinal) logistic regression model.\n\n::: l-body\n| Number | Question                                                             |\n|--------|----------------------------------------------------------------------|\n| 1      | What is the study aim?                                               |\n| 2      | What are the predictor(s)-of-interest and outcomes?                  |\n| 3      | What are the confounders - and why?                                  |\n| 4      | Why did the authors use both linear and ordinal regression analyses? |\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/jospt_rfd_table.png){fig-align='center' width=1100px}\n:::\n:::\n\n\n<aside>Paper can be found in `Articles` section.</aside>\n\n### Example 3[^2]\n\n[^2]: Bonus Example. Don't freak out if you don't understand this\n\nIndependent *t*-test is a special cases of simple linear regression\n\n$E(y|x) = \\alpha + \\beta x$\n\n$x$: Single binary predictor\n\n$\\beta$: the slope of a binary predictor ($x$) is equivalent to the difference in mean $y$ between 2 groups\n\nWhen you replace the binary predictor ($x$) by a multi-group/categorical predictor, linear regression and multi-group\nANOVA are identical. Hence, multi-group ANOVA is a special case of linear regression\n\n<aside>Because regression analysis generalizes many special tests (e.g., t-test, ANOVA, Wilcoxon Mann Whitney U test,\nKruskal-Wallis test), this explains our focus on regression analysis</aside>\n\n## Logistic Regression {#lrm}\n\n### The Challenge\n\n-   Logistic regression is a regression technique that is used to analyse binary outcome. \n\n-   Our response/outcome variable takes on 2 values, 0 or 1, and we convert them into the probability of a \"one\" response, given a set of predictors. \n\n-   We cannot use linear regression to predict probabilities because it may predict probabilities \\>1 or \\<0\n\n### The Workaround\n\n::: {.column-margin}\nFor the curious who wants to know why the exponentiated $\\beta$ is an OR. But don't freak out if you don't understand this!\n:::\n\n\n\n$$\n\\begin{split}\n\\text{Probabilty = }\\pi \\in [0,1] \\\\\n\\text{Odds = } \\frac{\\text{prob}}{1 - \\text{prob}} = \\frac {\\pi}{(1-\\pi)} \\in [0, +\\infty) \\\\\n\\text{log (odds) = }  \\text{log}\\left(\\frac{\\text{prob}}{1 - \\text{prob}}\\right) = \\text{log}\\left(\\frac {\\pi}{(1-\\pi)}\\right) \\in [-\\infty, +\\infty) \\\\\n\\log(\\text{odds}) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p \\\\ \n\\beta_1 = \\log(\\text{odds}_{x+1}) - \\log(\\text{odds}_x) \\\\\ne^{\\beta_1} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_x} = \\text{odds ratio [OR]} \n\\end{split}\n$$\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}