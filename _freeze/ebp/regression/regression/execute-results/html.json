{
  "hash": "cb9522c6257ea33416b8b58280452d98",
  "result": {
    "markdown": "---\ntitle: \"Regression\"\ndescription: |\n  Regression analysis is a statistical method that describes the relationship between a given predictor and outcome. In the case of multivariable regression, it describes that relationship after \"controlling\" for other variables. At the end of the session, you should be able to interpret the results from simple or multivariable regression (linear or logistic) analyses.\ndate: 07-16-2020\nauthor:\n  - name: SGH PT Dept\nbibliography: 'bibliography.bib'\ncategories:\n - regression\n - odds ratio\n - beta\nimage: 'images/ancova.png'\neditor: \n  markdown: \n    wrap: 120\n---\n\n\n\n\n## Definitions\n\n**linear regression model**: This is also called ordinary least squares (OLS) and refers to regression for a continuous\ndependent variable, and usually to the case where the residuals are assumed to be Gaussian.\n\n**multivariable model**: A model relating multiple predictor variables (risk factors, treatments, etc.) to a single\nresponse or dependent variable.\n\n**multivariate model**: A model that **simultaneously** predicts more than one dependent variable, e.g. a model to\npredict systolic and diastolic blood pressure or a model to predict systolic blood pressure 5 min. and 60 min. after\ndrug administration.\n\n<aside>Taken from Frank Harrell's [glossary](http://hbiostat.org/doc/glossary.pdf)</aside>\n\n## Regression Model\n\nThe choice of regression model depends on the nature of the outcome or dependent variable.\n\n-   Continuous outcome: linear regression model\n\n-   Binary outcome: binary logistic regression model\n\n-   Ordinal outcome: ordinal regression model\n\n-   Time to event outcome: Cox proportional hazards model[^1]\n\n[^1]: Not covered in this workshop\n\nRegression models are used for\n\n-   hypothesis testing\n\n-   estimation (with confounder adjustment to get adjusted estimates of effects)\n\n-   prediction\n\n\n\n```{mermaid}\n%%| column: screen-inset-right\n%%| fig-width: 8\n%%| fig-cap: \"Modified from Prof Harrell's [rmsc book](https://hbiostat.org/rmsc/intro.html)\"\nflowchart LR\nA[Goal of <br>Analysis] --> test[Hypothesis testing] & I[Interpretation<br>Effects Estimation] & Pred[Prediction]\nPred --> V[Validation]\nI --> festimat[Point and interval<br>estimation of <br>predictor's effect]\ntest --> ftest[Formal tests]\n\n```\n\n\n\n\n### Notation\n\n$E(y|x) = \\alpha + \\beta x$ (simple regression)\n\n$E(y|x) = \\alpha + \\beta_1x_1 + \\beta_2x_2$ (multiple regression)\n\n$y$: Response, dependent, or outcome variable\n\n$x$: Predictor or independent variable\n\n$E(y|x)$ : Expected value of $y$ conditioned on the value of $x$\n\n$\\beta$: Slope of $y$ on $x$ ($\\frac{\\Delta{y}}{\\Delta{x}}$)\n\n<!-- $e$: random error = residual = variation in $y$ when holding $x$ constant (e.g., variation in gait speed [$y$] between patients of the same age [$x$]) -->\n\n<aside>June says I have maxed out my quota for equations and symbols</aside>\n\nWatch this video which explains the notation of simple linear regression\n\n<aside>Use this [link](https://www.youtube.com/watch?v=KsVBBJRb9TE&list=PLvxOuBpazmsND0vmkP1ECjTloiVz-pXla) if the video\ndoesn't play</aside>\n\n<iframe width=\"560\" height=\"350\" src=\"https://www.youtube.com/embed/KsVBBJRb9TE\" frameborder=\"0\" allowfullscreen>\n\n</iframe>\n\n### Interpretating $x$\n\n-   Generally speaking, $x$ is the predictor or independent variable - a variable that is associated with the outcome\n    ($y$)\n\n-   Depending on the study design, $x$ can also represent the explanatory variable, risk factor, confounder, covariate,\n    or covariable. With multiple $x$'s, they can be (i) multiple risk factors, (ii) treatment variable plus baseline\n    outcomes plus patient descriptors (age, gender), or (iii) different levels of a nominal (categorical) variable\n    (e.g., race)\n\n### Interpretating $\\beta$\n\n-   slope\n\n-   regression coefficient\n\n-   effect size of a given predictor\n\n-   effect of increasing $x$ by one unit on the change in the mean of $y$, holding all other $x$'s constant\n\n-   with multiple $x$'s, the $\\beta$s represent the *partial* effects of $x$s\n\n$E(y|x) = \\alpha + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3$\n\n<aside>Note: By holding all other $x$'s (confounders) constant, we are minimizing the risk of confounding!</aside>\n\n$\\beta_1$ interpretation: Holding $x_2$ and $x_3$ constant, how much does mean $y$ change when $x_1$ changes by 1 unit?\n\n$\\beta_2$ interpretation: Holding $x_1$ and $x_3$ constant, how much does mean $y$ change when $x_2$ changes by 1 unit?\n\n$\\beta_3$ interpretation: Holding $x_1$ and $x_2$ constant, how much does mean $y$ change when $x_3$ changes by 1 unit?\n\nIQR-$\\beta$ = change in the mean of $y$ per one *IQR* increase in $x$\n\nWatch this video which explains the interpretation of $\\beta$ from a simple linear regression model. Please note that\n$\\beta_0$ in the video refers to $\\alpha$ using our notation.\n\n<aside>Use this [link](https://www.youtube.com/watch?v=I8Dr1OGUdZQ&list=PLvxOuBpazmsND0vmkP1ECjTloiVz-pXla&index=3) if\nthe video doesn't play</aside>\n\n<iframe width=\"560\" height=\"350\" src=\"https://www.youtube.com/embed/I8Dr1OGUdZQ\" frameborder=\"0\" allowfullscreen>\n\n</iframe>\n\nWatch this [video](https://www.youtube.com/watch?v=dQNpSa-bq4M) which explains the interpretation of $\\beta$ from a\nmultivariable linear regression model. Please note that $\\beta_0$ in the video refers to $\\alpha$ using our notation. To\njump straight into $\\beta$ interpretation, start from minute $14$ of the video.\n\nLogistic regression = exponentiating $\\beta$ = [Odds Ratio](#lrm)\n\nCox proportional hazards regression = exponentiating $\\beta$ = Hazard Ratio (HR)\n\n## Linear Regression\n\n### Example 1\n\nExpected post-Rx disability = $24$ + $0.71\\times$baseline score + $12.7\\times$acupuncture\n\n-   Interpretation: **12.7** represents the difference between groups on mean change scores. Outcome score improved by\n    an estimated **12.7** points more on average in the acupuncture group than in the placebo group.\n\n<aside>In a research paper, the authors will report the $\\beta$ for the `acupuncture` (*12.7*) term</aside>\n\n-   An analysis of covariance (ANCOVA) adjusts each patient's follow up score for his or her baseline\n    score[@vickers2001ancova]\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The estimated difference between the groups from analysis of covariance is the vertical distance between the red and dotted lines. This figure is taken from Vickers et al [@vickers2001ancova]](images/ancova.png){fig-align='center' width=100%}\n:::\n:::\n\n\n<aside>Disclaimer: Don't freak out if you don't understand this. As mentioned, the most important result is *12.7* - the\n$\\beta$ for acupuncture</aside>\n\n-   **ANCOVA**: Analysis of covariance is *just* multiple regression (i.e., a linear model ) where one variable is of\n    major interest and is categorical (e.g., treatment group). In classic ANCOVA there is a treatment variable and a\n    continuous covariate used to reduce unexplained variation in the dependent variable, thereby increasing power.\n\n<aside>Taken from Frank Harrell's [glossary](http://hbiostat.org/doc/glossary.pdf)</aside>\n\n### Example 2\n\nTable 2, taken from our JoSPT paper[@pua2017associations], describes how to interpret the output from a linear and\n(ordinal) logistic regression model.\n\n::: l-body\n| Number | Question                                                             |\n|--------|----------------------------------------------------------------------|\n| 1      | What is the study aim?                                               |\n| 2      | What are the predictor(s)-of-interest and outcomes?                  |\n| 3      | What are the confounders - and why?                                  |\n| 4      | Why did the authors use both linear and ordinal regression analyses? |\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/jospt_rfd_table.png){fig-align='center' width=1100px}\n:::\n:::\n\n\n<aside>Paper can be found in `Articles` section.</aside>\n\n### Example 3[^2]\n\n[^2]: Bonus Example. Don't freak out if you don't understand this\n\nIndependent *t*-test is a special cases of simple linear regression\n\n$E(y|x) = \\alpha + \\beta x$\n\n$x$: Single binary predictor\n\n$\\beta$: the slope of a binary predictor ($x$) is equivalent to the difference in mean $y$ between 2 groups\n\nWhen you replace the binary predictor ($x$) by a multi-group/categorical predictor, linear regression and multi-group\nANOVA are identical. Hence, multi-group ANOVA is a special case of linear regression\n\n<aside>Because regression analysis generalizes many special tests (e.g., t-test, ANOVA, Wilcoxon Mann Whitney U test,\nKruskal-Wallis test), this explains our focus on regression analysis</aside>\n\n## Logistic Regression {#lrm}\n\n### The Challenge\n\n-   Logistic regression is a regression technique that is used to analyse binary outcome. \n\n-   Our response/outcome variable takes on 2 values, 0 or 1, and we convert them into the probability of a \"one\" response, given a set of predictors. \n\n-   We cannot use linear regression to predict probabilities because it may predict probabilities \\>1 or \\<0\n\n### The Workaround\n\n::: {.column-margin}\nFor the curious who wants to know why the exponentiated $\\beta$ is an OR. But don't freak out if you don't understand this!\n:::\n\n\n\n$$\n\\begin{array}{ccc}\n\\text{Probabilty } &=&\\pi \\in [0,1] \\\\\n\\text{Odds} &=&  \\frac{\\text{prob}}{1 - \\text{prob}} &=& \\frac {\\pi}{(1-\\pi)} \\in [0, +\\infty) \\\\\n\\text{log (odds)}  &=& \\text{log}\\left(\\frac{\\text{prob}}{1 - \\text{prob}}\\right) &=& \\text{log}\\left(\\frac {\\pi}{(1-\\pi)}\\right) \\in [-\\infty, +\\infty) \\\\\n\\log(\\text{odds}) &=& \\log\\left(\\frac{\\pi}{1-\\pi}\\right) &=& \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p \\\\ \n\\beta_1 &=& \\log(\\text{odds}_{x+1}) - \\log(\\text{odds}_x) \\\\\ne^{\\beta_1} &=& \\frac{\\text{odds}_{x+1}}{\\text{odds}_x} &=& \\text{odds ratio [OR]} \n\\end{array}\n$$\n \n\n### Interpreting $\\beta$\nFocusing on the simple logistic regression model (1 predictor)\n$$\n\\log{\\left( \\frac{prob}{1-prob} \\right)} = \\beta_0 + \\beta_1 X  \\\\\n$$\n\nThe left-hand side of the logistic regression equation  $\\text{log}\\left(\\frac{\\text{prob}}{1 - \\text{prob}}\\right)$  is the natural logarithm of the odds, also known as the “log-odds” or “logit”  \n\n#### What is similar?\n- Recall that in linear regression, $\\beta$ is the difference in the outcome associated with a 1-unit difference in the predictor (X)   \n- **Similarly**, in logistic regression, $\\beta$  is the difference in the log-odds of the outcome associated with a 1-unit difference in the predictor (X)   \n\n<aside>\nAdapted from Ramzi W. Nahhas's [online book](https://bookdown.org/rwnahhas/RMPH/blr.html) \n</aside>\n\n\n\n$$\n\\begin{array}{ccc}\n\\beta_1 &=& Y_{x+1} - Y_x ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\text{linear regression}\\\\\n\\beta_1 &=& \\log(\\text{odds}_{x+1}) - \\log(\\text{odds}_x) ~~~~~~~~\\text{logistic regression}\\\\\n\\end{array}\n$$\n\n\n#### What is different and potentially confusing?\n- Because the logistic regression model is on the logged scale, when X is a continuous predictor, $\\beta$ is the log of the odds ratio comparing individuals who differ in that predictor by one unit      \n- Because the logistic regression model is on the logged scale, when X is a categorical predictor, $\\beta$ is the log of the odds ratio comparing individuals at a given categorical level of the predictor to those at the reference level.       \n- Because $\\beta$ is the log of the odds ratio, we exponentiate $\\beta$ to compute an odds ratio (OR) for X\n\n$$\n\\begin{array}{ccc}\n\\beta_1 &=& \\log(\\text{odds}_{x+1}) - \\log(\\text{odds}_x) &=& \\log{\\left(\\frac{\\text{odds}_{x+1}}{\\text{odds}_x}\\right)} &=& \\log(OR) \\\\  \ne^{\\beta_1} &=& \\frac{\\text{odds}_{x+1}}{\\text{odds}_x} &=& \\text{odds ratio [OR]}\n\\end{array}\n$$\n\n \n\n\n### Odds \n\n- Odds = Probability that an event or outcome will occur divided by the probability that it will not occur \n\n- If there probability of disability is 75% ($\\frac{3}{4}$), the odds of disability is $\\frac{3/4}{1-3/4} = 3$  \n\n- If the odds of disability is 3, the probability of disability is $\\frac{3}{1 + 3} = 0.75 \\text{ or } 75\\%$\n\n$$\n\\begin{array}{ccc}\n\\text{Odds = } \\frac{\\text{prob}}{1 - \\text{prob}} \\\\\n\\text{Prob = } \\frac{\\text{odds}}{1 + \\text{odds}}  \\\\\n\\end{array}\n$$\n\n<iframe width=\"560\" height=\"350\" src=\"https://www.youtube.com//embed/ARfXDSkQf1Y\" frameborder=\"0\" allowfullscreen></iframe>\n\n\n<aside>\nI strongly recommend that you watch the [Statquest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw) videos to gain a good basic understanding of biostatistics. \n</aside>\n\n### Odds Ratio\n\nBecause OR is a ratio of odds of the outcome    \n\n- An OR of 1 implies no association between the predictor and the outcome.  \n\n- An OR > 1 implies a positive association between the predictor and the outcome.       \n\n- An OR < 1 implies a negative association between the predictor and the outcome. \n\n\nExample   \n\nIf patients with knee pain have an odds of disability of 2.0 and patients without knee pain have an odds of disability of 0.5, then the OR associated with the presence of knee pain would be $\\frac{2.0}{0.5}$ = 4      \n\n- Patients with knee pain have 4 times the odds of disability compared with patients without knee pain. \n\n- When we reverse the order of the groups being compared, the OR will be inverted. So, patients without knee pain have $\\frac{1}{4}$ or 0.25 times the odds of disability compared with patients with knee pain        \n\n- Note: **Odds** is the ratio of probabilities whilst **Odds Ratio** is the ratio of the odds. Watch the first **5** minutes of this video\n<iframe width=\"560\" height=\"350\" src=\"https://www.youtube.com//embed/8nm0G-1uJzA\" frameborder=\"0\" allowfullscreen></iframe>\n\n\n\n\n### Example 1\n\nTable 2, taken from our Knee paper[@pua2017associations], describes how to interpret the output from an ordinal logistic regression model.\n \n::: l-body\n| Number           | Question                                    |\n|------------------|------------------------------------------------|\n| 1     | What is the study aim?                |\n| 2      | What are the predictor(s)-of-interest and outcomes?  |\n| 3     | What are the confounders - and why? |\n| 4  | Why did the authors use both linear and ordinal regression analyses?          |\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/knee_iso_table.png){fig-align='center' width=1100px}\n:::\n:::\n\n\n<aside>\nPaper can be found in `Articles` section.\n</aside>\n\n\n### Example 2\nConsider Assoc Prof. [Shamala Thilarajah's](https://twitter.com/sthilarajah) [PTJ paper](https://academic.oup.com/ptj/advance-article-abstract/doi/10.1093/ptj/pzab060/6134189)\n\n<aside>\nPaper can be found in `Articles` section.\n</aside>\n\n::: l-body\n| Number           | Question                                    |\n|------------------|------------------------------------------------|\n| 1     | Table 1: Based on the median, IQR, mean, and SD, What can you conclude about the distributions of the variables?              |\n| 2      | Table 2: Interpret all the statistics|\n| 3     | Table 3: Interpret all the statistics|\n| 4  | Table 3: Why did the authors use both linear and ordinal regression analyses?          |\n:::\n\n\n## For the Eager\n[Ewen Harrison](https://www.ed.ac.uk/surgery/staff/profiles/ewen-harrison), consultant surgeon and data-scientist, has a nice [online chapter](https://argoshare.is.ed.ac.uk/healthyr_book/regression.html) on linear regression for healthcare professionals\n\n<aside>\nCheck out his nice example of the association of systolic blood pressure with smoking status and coffee consumption   \n</aside>  \n\nOur team has also gone Bayesian in our analyses. See [here](https://pubmed.ncbi.nlm.nih.gov/35151629/), [here](https://pubmed.ncbi.nlm.nih.gov/36963864/), and [here](https://pubmed.ncbi.nlm.nih.gov/36940758/) for examples.  \n \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}